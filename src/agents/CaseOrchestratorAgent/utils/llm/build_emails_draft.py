import asyncio

from src.agents.CaseOrchestratorAgent.utils.llm.draft_to_llm_processing import normalize_and_dedupe_draft
from src.agents.CaseOrchestratorAgent.utils.llm.llm_parser import parse_llm_email_json


async def build_email_to_user(container, case_id, reviewer_note, draft_customer_reply):
    # 0) Clean the draft (remove repeats + remove intent dumps)
    clean_draft = normalize_and_dedupe_draft(draft_customer_reply or "")

    # 1) Load templates
    system_prompt = container.template_parser.get_template_from_locales(
        "final_reply_email", "final_system_prompt"
    )

    # 2) Build params prompt (include reviewer_note + clean draft)
    parms_prompt = container.template_parser.get_template_from_locales(
        "final_reply_email",
        "final_params_prompt",
        {
            "case_id": case_id,
            "reviewer_note": reviewer_note or "",
            "draft_customer_reply": clean_draft,
        },
    )

    # 3) Footer
    footer_prompt = container.template_parser.get_template_from_locales(
        "final_reply_email", "final_footer_prompt"
    )

    chat_history = [
        container.generation_client.construct_prompt(
            prompt=system_prompt,
            role=container.generation_client.enums.SYSTEM.value,
        )
    ]

    full_prompt = "\n\n".join([parms_prompt, footer_prompt])

    answer, total_tokens, cost = await asyncio.to_thread(
        container.generation_client.generate_text,
        full_prompt,
        chat_history,
    )


    subject_body = parse_llm_email_json(answer)

    if subject_body is None:
        # fallback: still use the cleaned draft
        subject = f"Re: Your request [CASE: {case_id}]"
        body = clean_draft
    else:
        subject, body = subject_body


    print(f"Final reply Email generated by LLM\n Subject:{subject}, Budy:{body}")
    print("="*20)

    return subject, body